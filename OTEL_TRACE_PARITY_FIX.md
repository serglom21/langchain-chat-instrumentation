# ğŸ”§ OpenTelemetry Trace Parity Fix

## ğŸ” Problem Analysis

Comparing the Sentry SDK trace with OpenTelemetry traces, we found **disconnected spans** instead of a unified trace hierarchy.

### Sentry SDK Trace Structure (Target)
```
http.server â€” /chat (958ms)
â””â”€â”€ Autogrouped â€” middleware.starlette (957ms)
    â””â”€â”€ workflow.execution â€” LangGraph Workflow Execution (952ms)
        â””â”€â”€ workflow.langgraph_invoke â€” LangGraph Graph Invoke (952ms)
            â””â”€â”€ gen_ai.invoke_agent â€” invoke_agent LangGraph (952ms)
                â”œâ”€â”€ node_operation â€” Node: input_validation (0.03ms)
                â”œâ”€â”€ node_operation â€” Node: context_preparation (0.08ms)
                â”œâ”€â”€ node_operation â€” Node: llm_generation (900ms)
                â”‚   â”œâ”€â”€ ai.chat â€” LLM Generation with OpenAI GPT-3.5-turbo (900ms)
                â”‚   â”‚   â”œâ”€â”€ ai.chat.invoke â€” LangChain LLM Invoke (900ms)
                â”‚   â”‚   â”‚   â”œâ”€â”€ ai.chat.preprocess â€” LangChain Message Preprocessing (0.01ms)
                â”‚   â”‚   â”‚   â”œâ”€â”€ ai.chat.generate â€” Streaming LangChain Generate Call (900ms)
                â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ai.chat.internal_processing â€” LangChain Internal Processing (900ms)
                â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ai.chat.invoke_overhead â€” LangChain Invoke Overhead (900ms)
                â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gen_ai.chat â€” chat gpt-3.5-turbo (899ms)
                â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ http.client â€” POST OpenAI API (513ms)
                â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ai.chat.post_http_processing â€” LangGraph Post-HTTP (0.04ms)
                â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ai.chat.langgraph_processing â€” LangGraph Internal (0.48ms)
                â”‚   â”‚   â””â”€â”€ ai.chat.process_response â€” Process LLM Response (0.17ms)
                â”œâ”€â”€ node_operation â€” Node: response_processing (0.05ms)
                â””â”€â”€ node_operation â€” Node: conversation_update (0.03ms)
```

### OpenTelemetry Current State (Problem)
```
âŒ Disconnected spans:
- Node: input_validation (isolated)
- Node: context_preparation (isolated)
- parent_span (test trace)
```

## ğŸ¯ Root Causes

### 1. Missing HTTP Server Root Span
**Issue**: Starlette instrumentation is applied but not creating proper root spans.

**Why**: The instrumentation needs to be applied BEFORE the app routes are defined, and we need to ensure the server span is the root.

### 2. Missing LangGraph Auto-Instrumentation
**Issue**: Sentry SDK has `gen_ai.invoke_agent` span from LangGraph integration, but OpenTelemetry doesn't.

**Why**: There's no official OpenTelemetry instrumentation for LangGraph yet. Sentry's integration is proprietary.

### 3. Missing LangChain Auto-Instrumentation
**Issue**: Sentry SDK has automatic `gen_ai.chat` spans, but OpenTelemetry doesn't.

**Why**: We're not using OpenTelemetry's LangChain instrumentation (if it exists), or we need to add it manually.

### 4. Context Not Propagating
**Issue**: Spans are created but not connected to parent context.

**Why**: The workflow execution might not be happening within the HTTP request span context.

## âœ… Solutions

### Solution 1: Fix HTTP Server Instrumentation

**File**: `otel_web_main.py`

**Problem**: Starlette instrumentation is applied after app creation.

**Fix**:
```python
# BEFORE (Wrong order)
app = Starlette(routes=[...])
StarletteInstrumentor.instrument_app(app)

# AFTER (Correct order)
from opentelemetry.instrumentation.starlette import StarletteInstrumentor

# Instrument BEFORE creating app
StarletteInstrumentor().instrument()

# Then create app
app = Starlette(routes=[...])
```

### Solution 2: Add HTTP Client Instrumentation for OpenAI

**Missing**: Automatic `http.client` spans for OpenAI API calls.

**Fix**: Add `requests` or `httpx` instrumentation:

```python
# In otel_config.py
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXInstrumentor

def setup_opentelemetry():
    # ... existing code ...
    
    # Add HTTP client instrumentation
    RequestsInstrumentor().instrument()
    HTTPXInstrumentor().instrument()
```

### Solution 3: Add Manual LangGraph Span

**Missing**: `gen_ai.invoke_agent` span that wraps the entire graph execution.

**Fix**: Update `otel_state_graph.py`:

```python
def process_chat(self, user_input: str, conversation_history: List[Dict[str, Any]] = None):
    """Process a chat message through the StateGraph workflow."""
    try:
        initial_state = {
            "user_input": user_input,
            "conversation_history": conversation_history or [],
            "error": None
        }
        
        # Add LangGraph agent span (matches Sentry SDK)
        with create_span(
            "invoke_agent LangGraph",
            "gen_ai.invoke_agent",
            kind=trace.SpanKind.INTERNAL
        ) as agent_span:
            agent_span.set_attribute("gen_ai.system", "langgraph")
            agent_span.set_attribute("gen_ai.operation.name", "invoke_agent")
            
            # Existing workflow execution span
            with create_span(
                "LangGraph Workflow Execution",
                "workflow.execution",
                kind=trace.SpanKind.INTERNAL
            ) as workflow_span:
                # ... rest of the code
```

### Solution 4: Add LangChain Callback Integration

**Missing**: Automatic `gen_ai.chat` spans from LangChain.

**Current**: We have `OpenTelemetryLangChainCallback` but it creates spans manually.

**Enhancement**: Ensure the callback spans are created with proper operation names:

```python
# In otel_chat_nodes.py - OpenTelemetryLangChainCallback

def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):
    """Called when LLM starts."""
    run_id = self._get_run_id(**kwargs)
    start_time = time.time()
    
    # Create span with EXACT operation name from Sentry SDK
    span = self.tracer.start_span(
        name=f"chat {serialized.get('name', 'unknown')}",  # "chat gpt-3.5-turbo"
        kind=trace.SpanKind.CLIENT,
    )
    
    # Use gen_ai.chat operation (matches Sentry)
    span.set_attribute("span.op", "gen_ai.chat")
    span.set_attribute("gen_ai.system", "openai")
    span.set_attribute("gen_ai.operation.name", "chat")
    # ... rest of the attributes
```

### Solution 5: Ensure Context Propagation

**Issue**: Spans might not be inheriting the HTTP request context.

**Fix**: Verify the tracer is using the current context:

```python
# In otel_web_main.py - chat_endpoint

async def chat_endpoint(request):
    tracer = get_tracer()
    
    # Get current span (should be the HTTP server span from Starlette)
    current_span = trace.get_current_span()
    
    # Verify we have a valid span context
    if not current_span.is_recording():
        print("WARNING: No active span context!")
    
    # All child spans will now inherit this context
    result = chat_graph.process_chat(user_input, conversation_history)
```

## ğŸ“ Complete Implementation

### Step 1: Update `otel_config.py`

Add HTTP client instrumentation:

```python
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXInstrumentor

def setup_opentelemetry() -> trace.Tracer:
    """Initialize OpenTelemetry with Sentry OTLP exporter."""
    global _tracer
    
    if _tracer is not None:
        return _tracer
    
    # ... existing setup code ...
    
    # Add HTTP client instrumentation for OpenAI API calls
    try:
        RequestsInstrumentor().instrument()
    except Exception as e:
        print(f"Warning: Could not instrument requests: {e}")
    
    try:
        HTTPXInstrumentor().instrument()
    except Exception as e:
        print(f"Warning: Could not instrument httpx: {e}")
    
    _tracer = trace.get_tracer(...)
    return _tracer
```

### Step 2: Update `otel_web_main.py`

Fix Starlette instrumentation order:

```python
from opentelemetry.instrumentation.starlette import StarletteInstrumentor

# Initialize OpenTelemetry FIRST
setup_opentelemetry()

# Instrument Starlette BEFORE creating app
StarletteInstrumentor().instrument()

# Initialize settings and chat graph
settings = get_settings()
chat_graph = OtelChatStateGraph(settings.openai_api_key)

# Create Starlette app (will be automatically instrumented)
app = Starlette(
    debug=True,
    routes=[...],
)

# DON'T call instrument_app() again - it's already instrumented
```

### Step 3: Update `otel_state_graph.py`

Add LangGraph agent span:

```python
def process_chat(self, user_input: str, conversation_history: List[Dict[str, Any]] = None):
    """Process a chat message through the StateGraph workflow."""
    try:
        initial_state = {
            "user_input": user_input,
            "conversation_history": conversation_history or [],
            "error": None
        }
        
        # Add LangGraph invoke_agent span (matches Sentry SDK)
        with create_span(
            "invoke_agent LangGraph",
            "gen_ai.invoke_agent",
            kind=trace.SpanKind.INTERNAL
        ) as agent_span:
            agent_span.set_attribute("gen_ai.system", "langgraph")
            agent_span.set_attribute("gen_ai.operation.name", "invoke_agent")
            agent_span.set_attribute("agent.type", "state_graph")
            
            # Workflow execution span
            with create_span(
                "LangGraph Workflow Execution",
                "workflow.execution",
                kind=trace.SpanKind.INTERNAL
            ) as workflow_span:
                workflow_span.set_attribute("initial_state_keys", str(list(initial_state.keys())))
                workflow_span.set_attribute("user_input_length", len(user_input))
                
                # Graph invoke span
                with create_span(
                    "LangGraph Graph Invoke",
                    "workflow.langgraph_invoke",
                    kind=trace.SpanKind.INTERNAL
                ) as graph_invoke_span:
                    graph_invoke_span.set_attribute("description", 
                        "LangGraph internal processing during graph.invoke()")
                    graph_invoke_span.set_attribute("state_keys", str(list(initial_state.keys())))
                    
                    result = self.graph.invoke(initial_state)
                    
                    # ... rest of the code
```

### Step 4: Update `requirements.txt`

Add HTTP client instrumentation:

```txt
# OpenTelemetry HTTP client instrumentation
opentelemetry-instrumentation-requests>=0.41b0
opentelemetry-instrumentation-httpx>=0.41b0
```

## ğŸ§ª Testing the Fix

### 1. Install New Dependencies

```bash
pip install opentelemetry-instrumentation-requests opentelemetry-instrumentation-httpx
```

### 2. Apply the Code Changes

Update the files as described above.

### 3. Run the Server

```bash
python otel_web_main.py
```

### 4. Send a Test Request

```bash
curl -X POST http://localhost:8002/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is 2+2?", "session_id": "test123"}'
```

### 5. Check Sentry

Expected trace structure:
```
POST /api/chat
â””â”€â”€ Autogrouped â€” middleware.starlette
    â””â”€â”€ invoke_agent LangGraph
        â””â”€â”€ LangGraph Workflow Execution
            â””â”€â”€ LangGraph Graph Invoke
                â”œâ”€â”€ Node: input_validation
                â”œâ”€â”€ Node: context_preparation
                â”œâ”€â”€ Node: llm_generation
                â”‚   â”œâ”€â”€ LLM Generation with OpenAI
                â”‚   â”‚   â””â”€â”€ chat gpt-3.5-turbo
                â”‚   â”‚       â””â”€â”€ http.client â€” POST OpenAI
                â”œâ”€â”€ Node: response_processing
                â””â”€â”€ Node: conversation_update
```

## ğŸ“Š Expected Results

### Before Fix
- âŒ Disconnected spans
- âŒ No HTTP root span
- âŒ No LangGraph agent span
- âŒ No automatic HTTP client spans

### After Fix
- âœ… Complete trace hierarchy
- âœ… HTTP server root span (from Starlette)
- âœ… LangGraph agent span (manual)
- âœ… HTTP client spans (from requests/httpx instrumentation)
- âœ… All spans connected in proper parent-child relationships

## ğŸ¯ Parity Checklist

Compare with Sentry SDK trace `40b5515620eb418c9ce9f216a724e07c`:

- [ ] HTTP server root span (`http.server â€” /chat`)
- [ ] Starlette middleware span (`middleware.starlette`)
- [ ] LangGraph agent span (`gen_ai.invoke_agent`)
- [ ] Workflow execution span (`workflow.execution`)
- [ ] Graph invoke span (`workflow.langgraph_invoke`)
- [ ] Node operation spans (5 nodes)
- [ ] LLM generation spans (ai.chat hierarchy)
- [ ] HTTP client span (`http.client â€” POST OpenAI`)
- [ ] Proper timing (total ~950ms)
- [ ] All spans connected in hierarchy

## ğŸš€ Next Steps

1. Apply all code changes
2. Install new dependencies
3. Test with real requests
4. Verify trace in Sentry dashboard
5. Compare span count and hierarchy with Sentry SDK trace
6. Adjust span names/operations to match exactly

