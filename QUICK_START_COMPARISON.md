# âš¡ Quick Start: Side-by-Side Comparison

## ğŸš€ 30-Second Setup

```bash
# 1. Set environment variables
export OPENAI_API_KEY='your-key-here'
export SENTRY_DSN='your-dsn-here'

# 2. Start both servers
./compare_both.sh

# 3. Open both UIs in separate tabs
# - http://localhost:8000 (Purple - Custom)
# - http://localhost:8001 (Orange - Baseline)
```

## ğŸ“ Quick Test

1. **Send the same message to both**: "What is Python?"
2. **Go to Sentry â†’ Performance â†’ Traces**
3. **Filter by environment**:
   - `production` = Custom
   - `production-baseline` = Baseline
4. **Compare the traces**

## ğŸ‘€ What You'll See

### Custom (Purple, Port 8000)
- âœ… 15+ spans showing complete workflow
- âœ… Node-level timing
- âœ… Token timing metrics
- âœ… Business context tags
- âœ… Framework overhead visible

### Baseline (Orange, Port 8001)
- âŒ Only 2 spans (HTTP + API call)
- âŒ No workflow visibility
- âŒ No token timing
- âŒ No business context
- âŒ Framework overhead hidden

## ğŸ¯ Key Differences

| What | Custom | Baseline |
|------|--------|----------|
| Spans | 15+ | 2 |
| Can find bottlenecks? | âœ… Yes | âŒ No |
| Token timing? | âœ… Yes | âŒ No |
| Business metrics? | âœ… Yes | âŒ No |
| Debug errors easily? | âœ… Yes | âŒ No |

## ğŸ’¡ The Point

**Auto-instrumentation alone is NOT enough for AI/LLM applications.**

You need custom instrumentation to see:
- Workflow execution
- Token-level performance
- Business context
- Framework overhead
- Granular errors

## ğŸ“š Learn More

- [COMPARISON_GUIDE.md](COMPARISON_GUIDE.md) - Detailed comparison instructions
- [COMPARISON_SUMMARY.md](COMPARISON_SUMMARY.md) - Visual comparison
- [README.md](README.md) - Full documentation


